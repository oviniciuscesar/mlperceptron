Network Structure:
Parameters: 7
Number of layers: 5
5 6 20 15 10 5 2 
Activation Functions:
prelu tahn tah sigmoid softmax 
Hyperparameters:
Learning rate: 0.01
Max epochs: 5000
Current epoch: 5001
Training mode: 0
Data size: 60
Error: 859712
Layer 1 Weights:
0.703202 -0.0667643 0.431413 0.769385 1.03398 -0.840099 
0.862135 1.07179 0.353864 0.836289 1.16331 -0.321429 
1.62605 0.0891263 0.0956533 0.264284 -0.390816 -0.575123 
-0.204498 -0.0470974 0.582842 0.472562 0.778157 -0.411174 
0.554699 1.09525 -1.45937 -0.547161 -1.74385 -0.723819 
-1.73498 -0.0544073 -0.896013 -1.6349 1.41476 -1.06568 
-0.321777 -0.914325 -0.608332 0.616708 -0.854371 0.654975 
-0.327528 -0.247148 0.478422 -0.868821 0.826823 1.34982 
-1.23917 -1.35156 -0.688055 -0.622948 0.0337408 0.401634 
0.638351 -1.028 0.0493647 -0.710309 -0.889402 -0.912589 
0.445543 0.428881 0.875635 -0.19146 -0.71451 0.573 
-0.207863 1.37189 0.6621 0.76125 -1.03415 -1.30495 
-0.188542 0.202166 0.00643611 0.910761 1.23008 1.48585 
-0.403889 -0.896181 0.638253 -0.667847 1.19283 0.0625805 
-1.16402 -0.720406 1.43988 0.390417 -0.411874 0.364491 
0.285482 -1.15005 1.12912 -0.388568 -0.661312 -0.861657 
-1.58016 0.864017 -0.928631 0.702783 0.437704 -0.169624 
-0.226997 -0.140826 -0.242992 0.0412041 1.06304 0.0930938 
0.0699029 -0.535856 -0.267691 -0.494887 -1.03728 -1.07181 
-0.765964 0.685769 -0.242594 0.715101 0.307301 1.46669 
Layer 2 Weights:
0.396653 -0.31715 0.115242 -1.01726 -0.405156 -0.757801 0.224321 0.523168 0.642978 1.11183 0.89381 0.617765 0.236166 -0.201289 -0.0301019 0.451416 -0.755539 0.151662 -0.680482 0.0302745 
-0.331816 -0.861307 -0.923187 -0.719641 -0.429976 0.304979 0.825146 -0.264214 -0.900545 0.0802266 -0.167443 0.747125 1.05929 -0.967402 -0.864784 -0.469523 0.0578266 -0.112164 0.359718 0.418598 
0.0639968 0.928325 0.498991 -0.147932 -0.718269 0.750716 -0.450837 -0.286442 0.779008 -0.906593 0.569567 0.111909 0.162913 -0.519407 0.317659 0.263049 -0.835977 -0.0986472 -0.746148 -0.390389 
-1.02254 -0.283462 -0.630261 0.241976 1.05211 -0.744577 -0.312296 0.145749 -0.310721 -0.0561437 -1.30358 1.35388 -0.511231 -0.0769049 -0.123933 0.621688 -0.170222 -0.383128 0.137129 -1.53685 
-0.15718 -0.291079 -0.53577 -0.633855 0.096403 -1.10087 -0.715357 0.516499 0.904552 0.247908 0.142108 0.464675 0.520552 0.893539 0.023985 0.973853 0.923594 -0.909507 -0.713797 0.613091 
0.51928 -1.17057 -0.621675 -0.680761 0.742703 -0.319762 -0.909894 0.650112 0.466427 -0.33181 -0.615773 1.18828 0.603407 -0.259295 1.28061 -0.474615 0.753194 -0.736803 -0.830852 0.570269 
-0.771756 0.330588 -1.01209 -0.225055 0.0502985 -0.201202 -0.69966 -0.0219099 0.691354 -0.494245 -0.467572 1.01889 -0.638851 -0.419274 0.847448 0.090163 0.834492 -0.177468 -1.10435 0.157319 
-0.155825 0.539544 0.6431 -0.750042 -1.43845 1.98018 -0.979826 -1.39769 -0.736122 0.570731 1.02332 -0.345565 0.367277 1.1827 0.46227 -0.73611 -0.147821 -0.526141 -0.909273 -0.657867 
0.562363 -0.356514 0.505966 -0.489514 -0.873237 1.25721 0.592553 -0.236305 -0.486624 0.746542 0.627813 -0.751672 0.705711 -0.583457 -1.03768 -0.485432 0.179974 -0.213244 -0.817314 -0.303178 
0.73359 -0.923794 -0.298347 -0.424294 -0.0233602 -0.378731 0.478092 0.710502 -0.630417 -0.781858 -0.296557 -0.513008 -0.0190397 0.8711 -1.13516 0.0938097 0.165278 -0.0268201 0.769074 0.437441 
0.588475 -0.363989 -0.273443 0.472947 0.686838 1.40973 -0.35008 -1.04168 -0.5427 -0.113997 0.737126 -0.164304 -0.547408 0.638546 -0.155017 0.310702 -0.353873 0.238745 0.0583878 -0.133577 
0.81353 -0.0387594 0.969351 -0.217034 -0.198446 -0.20314 -0.536668 0.631238 -0.00270091 0.895108 -0.852609 0.557713 0.919156 -0.327673 -0.581155 0.255725 -0.521708 0.215783 0.83776 0.9186 
0.991247 0.336252 -0.405648 -0.792937 -0.841599 0.366996 0.214106 -0.114442 -1.56347 1.00089 0.431487 -1.48324 1.37235 -0.941069 -0.320189 -0.651874 0.334502 1.07966 -0.249346 0.828416 
-0.330747 -0.146956 -0.854703 0.890963 0.153222 -0.887855 0.255994 0.669128 0.218931 -0.274216 -0.662456 0.906726 -0.996736 0.528666 1.06328 0.0553642 0.916625 -0.315424 -1.13013 -0.60249 
-0.921077 -0.706618 -0.263721 0.102215 1.05056 0.750142 -0.941916 -0.00696801 -0.218969 -1.23902 -0.235658 -0.646385 -0.325489 0.422196 0.837852 -0.354588 0.289851 -0.41537 -0.73229 0.595471 
Layer 3 Weights:
0.334028 1.2091 0.571378 -1.2973 -1.26278 -1.19981 0.301222 1.49918 0.972341 0.541571 0.543544 -0.277618 1.03196 -1.36492 -1.24645 
0.114469 -0.873106 0.148239 1.13875 0.465778 0.442358 0.706296 -1.60027 -1.5783 -0.731803 0.24624 -0.595956 -0.851385 1.36104 1.02505 
0.478585 -0.194229 0.307255 -0.864013 0.0412479 -0.334624 0.193705 -0.0701103 0.414016 0.0387821 0.76838 0.827329 -0.194074 0.275637 -0.932188 
0.709949 -0.819659 -0.050152 -0.258554 -0.670251 -0.701709 0.180984 1.39361 -0.262171 0.644425 -0.585439 -0.499897 0.53907 -0.654226 -1.02869 
-0.172116 -0.24071 0.834425 1.08596 0.632608 -0.319022 0.430443 -0.974701 -0.764292 0.364576 0.300655 -0.89379 0.33323 -0.448723 -0.089346 
0.0999144 0.400798 0.58717 -0.819247 -0.943758 -0.625758 -1.20954 0.734874 0.771685 -0.432603 0.867175 0.102765 1.71184 -0.970944 -0.63469 
-0.263676 1.18169 -0.824142 -0.388549 -0.856148 -0.950478 -1.13764 0.593784 -0.203253 0.280471 1.01673 0.985336 1.30402 0.222249 -1.09118 
0.0559042 0.86817 -0.0590123 -0.266376 -0.918171 -1.01358 -0.539077 1.43469 0.799872 -0.489136 0.141506 -0.0278172 -0.504051 0.411816 0.613859 
0.0944662 -1.15419 0.765325 1.56002 0.111627 0.242305 0.361712 -1.56185 -0.819451 -0.0406228 -0.198458 -0.764444 -1.47397 1.62025 1.38994 
-0.742112 0.741428 0.250625 1.09626 0.306106 0.242759 -0.316897 -0.295092 -0.98967 0.689813 -0.230519 0.131412 -1.5209 0.81631 -0.270828 
Layer 4 Weights:
-0.743007 -0.140085 0.529791 -0.691625 -0.806571 -0.748019 -1.00299 0.632464 0.731211 -0.646945 
-1.61953 2.20042 -0.710544 -0.356074 0.90386 -1.80463 -0.995899 -0.977474 1.91416 0.706068 
0.412225 1.02623 -0.456252 0.918471 -0.47803 0.0744758 -0.396051 -0.695305 -0.550137 0.314015 
2.13396 -2.52333 0.735123 0.454732 -1.42111 1.56145 2.03255 1.10415 -2.08754 -1.90472 
-1.97962 0.760978 0.362795 -1.01938 -0.502514 -1.49933 -0.326539 0.403026 2.05599 0.359416 
Layer 5 Weights:
1.39864 2.8027 -0.730071 -4.48238 2.3871 
0.106401 -3.60889 -0.770604 4.64803 -2.2661 
Layer 1 Bias:
-0.42969 -0.126014 -0.261018 0.114289 0.423374 0.0235223 0.156356 -0.375319 -0.487718 -0.339143 0.431351 -0.0690742 0.389492 0.063351 -0.455177 -0.407794 -0.133353 -0.412326 -0.280875 -0.0344039 
Layer 2 Bias:
0.0621562 -0.0171612 -0.0391145 0.248895 -0.338109 0.290222 0.0411271 0.464846 0.335439 -0.0544778 0.211408 -0.037313 -0.0730955 -0.346304 -0.175601 
Layer 3 Bias:
-0.0174696 0.183909 -0.177085 0.182686 0.0177246 -0.0677107 0.0973999 0.109161 0.422138 0.173426 
Layer 4 Bias:
0.153488 0.421492 -0.0349119 -0.205658 0.271632 
Layer 5 Bias:
-0.457208 0.464674 
