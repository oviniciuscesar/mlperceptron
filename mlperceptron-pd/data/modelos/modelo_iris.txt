Network Structure:
Parameters: 5
Number of layers: 3
3 4 6 4 3 
Activation Functions:
prelu tahn softmax 
Hyperparameters:
Learning rate: 0.01
Max epochs: 10000
Current epoch: 0
Training mode: 0
Data size: 90
Error: 4.75352e+06
Layer 1 Weights:
1.02476 4.31218 -5.36615 -3.60715 
-0.438372 0.169475 -0.874364 -0.551549 
0.623985 -4.25062 0.905537 8.51699 
1.3508 -0.637665 -0.37151 0.559897 
1.23226 -2.31177 0.902687 -3.85005 
-0.609274 0.164959 -0.714764 0.639382 
Layer 2 Weights:
-0.54962 1.18669 -0.524505 -0.999766 -0.344577 -0.657679 
-0.676552 0.639676 1.78193 2.57181 0.486235 -1.04576 
3.66135 0.362661 -2.91997 -0.548381 0.0547955 0.209285 
-1.16703 -0.084496 3.60367 -1.85825 -3.63744 0.535155 
Layer 3 Weights:
-0.709087 -9.11501 8.66973 0.644212 
0.237828 5.99122 2.64425 -4.97468 
-0.0753617 4.11499 -11.099 5.10275 
Layer 1 Bias:
3.84548 0.0152013 -9.25474 -3.91616 1.71942 0.00588435 
Layer 2 Bias:
-0.199816 0.516324 5.81006 -0.363598 
Layer 3 Bias:
-1.64384 -2.8282 4.49457 
