Network Structure:
Parameters: 6
Number of layers: 4
4 6 12 6 4 2 
Activation Functions:
prelu tahn sigmoid softmax 
Hyperparameters:
Learning rate: 0.01
Max epochs: 3000
Current epoch: 3001
Training mode: 0
Data size: 60
Error: 1.15242e+06
Layer 1 Weights:
1.10234 -0.16066 -0.325865 0.481348 1.19708 -0.417691 
-0.484926 -0.14499 1.10743 1.63624 -2.21058 -1.79908 
2.21234 1.20659 -1.6278 0.0677556 0.857949 0.365594 
1.87992 0.129207 0.874096 -0.444733 0.228541 -0.598054 
0.0620411 -0.438422 -1.00248 0.468913 1.6071 -1.05645 
0.0116067 -1.98674 -0.379632 -0.806147 -0.625441 -2.05584 
-0.83011 0.945637 0.533945 -0.338398 0.544348 0.388454 
-0.798107 0.125932 -1.00267 0.759308 0.479616 1.68087 
0.500342 0.224448 0.543149 0.997895 0.634524 1.19158 
-0.347969 -1.26693 0.0824219 -1.03674 0.661613 1.6998 
-0.187415 0.337413 -1.02023 0.872495 0.382612 0.199217 
1.75809 1.37902 0.302373 -1.38186 1.42075 1.51005 
Layer 2 Weights:
-0.238958 0.357232 -0.377607 0.106131 -0.707638 -0.8709 -0.646307 -0.174883 -0.712749 -0.817822 0.0950618 -0.224545 
0.251045 -1.0387 -1.40834 1.30141 1.3145 1.73205 -0.928546 0.703615 1.24133 -0.743311 0.328711 0.492184 
-0.82228 -1.63681 -1.14487 1.03529 1.34479 -0.24504 -0.373073 0.28926 -0.427688 -0.64142 -0.777864 0.815455 
0.842368 -1.90623 -0.923878 1.20485 0.467326 -0.393235 -0.364875 1.214 0.354935 1.50846 0.862002 -0.199853 
-0.27413 -0.320638 -1.01478 0.963591 1.13408 -0.333757 -0.549615 -0.00290814 1.42391 -0.380091 0.554581 0.52974 
-1.07214 2.325 1.71627 -0.845062 -0.0801265 -2.7052 0.152111 -1.22508 0.0110327 1.75776 1.14574 -2.74836 
Layer 3 Weights:
0.17998 0.407409 1.38668 1.22792 1.03274 -2.90942 
0.251221 0.701238 2.38913 2.20159 2.25068 -4.49552 
0.550484 0.617264 -0.0985157 -1.35234 -0.45995 0.255521 
0.803728 -1.28231 -2.35662 -1.5569 -0.997889 3.5568 
Layer 4 Weights:
-2.38056 -4.69726 1.00091 4.77675 
2.55334 4.64421 -0.566494 -3.57249 
Layer 1 Bias:
0.0531704 -1.30158 0.460932 -0.444246 -0.181404 -1.60151 0.635787 0.678939 0.338728 -0.466361 -1.38166 0.256136 
Layer 2 Bias:
-0.106108 -0.326201 1.52178 0.449953 -0.000367014 0.124144 
Layer 3 Bias:
-1.01947 -2.44034 -0.0507067 2.06162 
Layer 4 Bias:
1.48025 -1.46933 
