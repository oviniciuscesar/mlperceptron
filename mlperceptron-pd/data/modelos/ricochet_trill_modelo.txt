Network Structure:
Parameters: 7
Number of layers: 5
5 6 20 15 10 5 2 
Activation Functions:
prelu tahn tah sigmoid softmax 
Hyperparameters:
Learning rate: 0.02
Max epochs: 7000
Current epoch: 7001
Training mode: 0
Data size: 40
Error: 1.36522e+06
Layer 1 Weights:
0.117293 0.341689 -0.128642 0.604444 1.13272 1.05503 
1.01511 0.253515 0.551622 0.0144341 0.47044 0.332279 
0.735122 -0.485877 4.78912 3.37497 -0.469081 2.55443 
1.32022 0.387283 0.349574 1.53658 1.18167 1.87565 
1.77007 1.00564 -0.536624 -0.140536 -0.305926 0.937827 
0.295798 0.424183 0.0426702 -0.648494 1.03129 0.37071 
1.23809 0.58636 0.758898 -0.296287 0.434898 0.404919 
1.55239 0.747313 -0.498017 -0.00359691 -0.32517 0.781251 
-0.311588 -2.12058 0.230609 -0.816243 -0.45174 -1.80685 
0.515816 0.474854 0.815144 0.387666 0.650159 0.812275 
0.350243 0.225844 0.1915 0.75787 0.878033 1.04752 
0.887775 0.368997 0.830082 0.398998 0.385901 0.702711 
1.39488 0.522287 0.0437889 0.474936 0.340388 1.10177 
0.477126 -0.016498 0.590089 0.447223 0.858782 0.688402 
0.211192 0.208965 -0.212258 0.831332 0.456564 0.224623 
0.757937 0.184855 0.519007 1.03449 0.516947 0.898609 
0.909313 0.32608 0.841591 0.393501 0.70452 0.718344 
0.429137 0.189011 0.286713 0.822546 0.850179 1.04922 
1.16218 0.457436 0.598496 0.333266 0.649476 0.874246 
0.508681 0.0627152 -0.453273 -0.12323 1.47651 -0.364448 
Layer 2 Weights:
0.717267 0.0994866 1.02364 0.0454981 0.489987 0.365767 0.700773 -0.00881946 0.899366 0.88979 0.0982553 1.02672 0.328478 0.945726 0.180857 0.868108 0.102613 0.372969 0.377738 0.821664 
1.02655 0.977722 -0.613749 0.858065 0.889091 0.77665 0.0859937 0.944408 0.916746 0.820096 0.690902 0.275287 0.105813 0.514786 0.948353 0.993239 0.863683 0.298034 0.347587 0.532351 
0.87595 0.537725 -0.723477 0.76451 0.954196 0.862219 0.833012 0.396406 0.201833 -0.0110489 0.56562 0.223402 0.805797 0.070409 0.297083 0.832686 0.850359 0.424185 0.588354 0.259165 
0.377558 0.92937 4.75049 -0.157162 0.87716 0.18658 0.687383 1.125 1.05609 0.390625 0.38386 0.0794854 0.715339 0.198772 0.244642 0.397477 0.535384 0.112403 0.558246 0.534047 
0.788437 0.384004 0.629148 0.772732 0.381413 0.513939 0.143403 0.90667 0.966509 0.537575 0.177825 0.995643 0.297767 0.937965 0.0805304 0.55129 0.698823 0.653186 0.311473 0.60289 
0.23114 0.757304 1.08002 0.130325 0.466383 0.29738 0.406733 0.858652 0.270851 0.461699 0.0893457 0.708013 0.165283 0.671315 0.49499 0.745153 0.0220162 0.773127 0.435345 0.531176 
0.556571 0.186101 -0.510765 1.03827 1.16579 0.542075 1.00543 1.00892 1.52118 0.0816951 0.384429 0.51871 0.562795 0.909857 0.580016 1.05823 -0.0531743 1.01392 0.294789 0.477869 
0.15022 0.794921 1.02386 0.72073 0.534485 0.77498 0.334038 0.60981 0.955764 -0.0214028 0.405958 0.357377 0.747882 0.149589 0.174714 0.163598 0.110026 0.622532 0.528841 0.460476 
1.21583 0.739678 -0.372982 1.24163 1.54153 1.20737 0.871086 1.05896 1.97596 0.173474 0.999969 0.0826367 0.488689 0.449908 0.353327 0.546359 0.532038 1.06109 1.11699 0.522118 
0.804921 0.379947 -0.560917 0.680937 0.670916 0.129665 0.835205 0.445852 0.235646 0.705943 0.991729 0.765523 0.896651 0.0759189 0.354151 0.346475 0.361366 0.146802 0.926094 0.746907 
0.0528055 0.529181 0.861265 -0.0752742 0.911877 0.733817 0.128913 0.0846823 -0.00362309 0.857193 0.264269 0.778405 0.595156 0.401909 0.0380822 0.082659 0.320757 0.371455 0.693695 0.601082 
0.704707 0.880115 4.0887 -0.267618 0.881513 0.077151 0.100876 0.944182 1.11389 0.512632 0.232268 0.184848 0.715746 0.562083 0.0726312 0.443857 0.0380564 0.275119 0.485995 0.914376 
0.443756 0.247654 -0.457283 0.911792 0.985628 0.798434 0.800208 0.932341 1.6287 0.167219 1.05077 0.920115 0.696884 0.798056 0.550005 0.429914 -0.0603034 1.04173 0.290476 0.8983 
0.284534 0.22077 1.50221 0.394068 0.939242 0.476686 0.510009 0.731409 0.596919 0.677068 0.87294 0.783765 0.526363 0.603683 0.94881 0.821331 0.319705 0.926704 0.39354 0.534081 
0.913222 0.118262 -0.6034 0.528302 0.986203 0.452388 0.44978 0.453224 1.01187 0.814493 0.356997 0.683128 0.698807 0.757351 0.809955 0.709307 0.521018 0.539875 0.366276 0.835468 
Layer 3 Weights:
-1.866 1.80546 1.47286 4.0391 -1.35561 -1.57578 2.66494 -1.22736 3.1533 0.853307 -1.41763 3.12972 2.74924 -0.261709 1.47264 
0.244829 0.814688 0.449743 0.148361 0.822064 0.779177 0.730692 0.379352 0.349632 0.980824 0.52686 0.497211 0.428171 0.870064 0.921057 
0.719188 0.499572 0.458065 0.665186 0.728803 0.504855 0.10775 0.0488299 0.224631 0.105016 0.702569 0.277611 0.251525 0.261276 0.807784 
0.697277 0.919918 0.786124 0.530827 0.801732 0.139132 0.262895 0.728489 0.945262 0.651989 0.988563 0.52176 0.145766 0.952019 0.138393 
0.186327 0.219544 0.54321 0.540774 0.212514 0.0904431 0.873009 0.614021 0.682011 0.977951 0.310339 0.712612 0.804473 0.649234 0.0331618 
0.634811 0.00109296 -0.338678 1.77783 -0.00366315 0.54519 0.272436 0.805781 1.35272 -0.00814746 -0.0838845 1.38149 0.433849 1.23987 0.253654 
0.304163 0.844187 0.928259 0.165446 0.977982 0.27205 0.877971 0.831992 0.441235 0.360564 0.440233 0.75404 0.326728 0.802527 0.915347 
0.215609 0.140013 0.3012 0.777741 0.742258 0.376156 0.54704 0.455319 0.722426 0.602691 0.192469 0.467739 0.151281 0.161829 0.939284 
0.264537 -0.147544 0.484455 0.317296 0.141547 -0.0663691 0.0703138 0.325024 0.822691 0.726439 0.0842455 0.00713354 0.477319 0.197706 0.11698 
0.939209 0.96737 0.437187 -0.0424144 0.867119 0.484805 0.757687 0.890372 0.467183 0.820831 0.464174 -0.0157251 0.970499 0.76173 0.20229 
Layer 4 Weights:
-0.862134 0.0929378 -0.45052 0.0966131 0.0275148 -0.519344 -0.497405 0.263135 0.615323 -0.220677 
1.08364 -0.466491 -0.147799 -0.700405 -0.476362 -0.400683 -0.474014 -0.297372 -0.322322 0.0264193 
-0.20765 -0.0978454 0.0659686 0.863622 0.655792 0.514982 0.32373 0.844023 0.629378 0.728497 
0.319514 0.119719 -0.0830501 -0.643866 -0.618184 -0.621648 0.19147 0.231689 -0.217846 -0.458025 
8.62658 -0.761504 -0.275381 -1.12349 -0.156152 2.87688 -1.08847 -0.354687 0.289903 -0.71665 
Layer 5 Weights:
0.630476 -0.223234 2.38826 0.339072 -4.14236 
-0.360513 0.495125 -1.70892 0.408029 5.67442 
Layer 1 Bias:
-0.409168 -0.25146 -2.05715 -0.229375 -1.07406 -0.22195 -0.300675 -0.958054 -1.95929 -0.529568 -0.335448 -0.338829 -0.590553 -0.331241 -0.66286 -0.423529 -0.478212 -0.346766 -0.278672 -0.905935 
Layer 2 Bias:
-1.01284 -0.192738 0.111441 -1.46888 0.0810649 -1.13622 -0.753382 -1.03754 -1.66759 -0.47455 -0.682303 -1.02046 -1.23394 -0.412287 -0.255448 
Layer 3 Bias:
-4.2754 0.0861054 -0.0497327 0.256194 -0.0361539 -2.21584 0.240132 0.0939575 -0.686652 0.0218921 
Layer 4 Bias:
-0.594463 -0.895448 -0.118856 -0.757759 -1.5904 
Layer 5 Bias:
1.95957 -1.95279 
